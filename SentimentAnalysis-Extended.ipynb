{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "### Gensim - 3.4.0\n",
    "### Pandas - 0.23.4\n",
    "### Numpy - 1.15.4\n",
    "### NLTK - 3.4\n",
    "### Contractions - 0.0.52\n",
    "### Pytorch - 1.9.0+cu102.\n",
    "### Note: The RNN models might give bad accuracies the first time. Re-Running the cell should give accuracies close to what I have mentioned in this file. I have attached screenshots of the accuracies for all the models as I was not able to run the entire model at once. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "<ipython-input-2-3b3fd54832d9>:9: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import contractions\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gensim.downloader as api\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gensim.models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import multiprocessing\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.tsv\", sep='\\t', error_bad_lines=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping N/A Values in Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['star_rating','review_body']]\n",
    "df = df.dropna(subset=['star_rating'])\n",
    "df = df.dropna(subset=['review_body'])\n",
    "df['star_rating'] = df['star_rating'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning Sentiment based on Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(star_rating):\n",
    "    if star_rating>3:\n",
    "        return 0\n",
    "    elif star_rating<3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "df['Sentiment'] = [sentiment(x) for x in df['star_rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling 50k reviews for each rating to create a dataset of 250k Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df[df['star_rating'] == 1].sample(50000)\n",
    "df_2 = df[df['star_rating'] == 2].sample(50000)\n",
    "df_3 = df[df['star_rating'] == 3].sample(50000)\n",
    "df_4 = df[df['star_rating'] == 4].sample(50000)\n",
    "df_5 = df[df['star_rating'] == 5].sample(50000)\n",
    "\n",
    "frames = [df_1, df_2, df_3, df_4, df_5]\n",
    "df_sampled = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sampled.to_csv('250kSamples.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sampled = pd.read_csv('250kSamples.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the GoogleWord2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Semantic Relationships of the Word Vectors using the GoogleWord2Vec Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ('card', 'poker'),  \n",
    "    ('card', 'credit'),  \n",
    "    ('card', 'birthday'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))\n",
    "    \n",
    "result = wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)\n",
    "result = wv.most_similar(positive=['woman', 'prince'], negative=['man'], topn=1)\n",
    "print(result)\n",
    "result = wv.most_similar(positive=['splendid', 'horrible'], negative=['good'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting all Reviews to Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled['review_body'] = df_sampled['review_body'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing HTML and URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_HTML(review):\n",
    "    cleanText = BeautifulSoup(review, \"html.parser\").text\n",
    "    return cleanText\n",
    "\n",
    "df_sampled['review_body'] = df_sampled['review_body'].apply(lambda review : remove_HTML(str(review)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping only Alphanumeric Characters in the Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('[^a-zA-Z \\']')\n",
    "df_sampled['review_body'] = df_sampled['review_body'].apply(lambda review : regex.sub('', review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting Extra spaces in the Reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('[ +]')\n",
    "df_sampled['review_body'] = df_sampled['review_body'].apply(lambda review : regex.sub(' ', review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contractionfunction(s):\n",
    "    return contractions.fix(s)\n",
    "df_sampled['review_body'] = df_sampled['review_body'].apply(lambda review : contractionfunction(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopWords(review):\n",
    "    tokens = word_tokenize(review)\n",
    "    filtered_words = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "df_sampled['review_body'] = df_sampled['review_body'].apply(lambda review : remove_stopWords(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing the Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WhitespaceTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(review):\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w) for w in tokenizer.tokenize(review)]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "df_sampled['review_body'] = df_sampled['review_body'].apply(lambda review : lemmatize_text(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Words from Reviews which are not part of the GoogleWord2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningfulWords = set(wv.vocab)\n",
    "def removeUnecssaryWords(review):\n",
    "    tokens = word_tokenize(review)\n",
    "    cleanReview = [word for word in tokens if word.lower() in meaningfulWords]\n",
    "    return cleanReview\n",
    "    \n",
    "df_sampled['review_body'] = df_sampled['review_body'].apply(lambda review : removeUnecssaryWords(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our ownWord2Vec Model using Gensim\n",
    "### Refrences: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "###                     https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "print(cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(df_sampled['review_body'], window=11, min_count=10, workers=cores-1, size=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Words from Reviews which are not part of our Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningfulWords = set(model.wv.vocab)\n",
    "def removeUnecssaryWords(review):\n",
    "    cleanReview = [word for word in review if word.lower() in meaningfulWords]\n",
    "    return cleanReview\n",
    "    \n",
    "df_sampled['review_body'] = df_sampled['review_body'].apply(lambda review : removeUnecssaryWords(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ('card', 'poker'),  \n",
    "    ('card', 'credit'),\n",
    "    ('card', 'birthday'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, model.wv.similarity(w1, w2)))\n",
    "result = model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)\n",
    "result = model.wv.most_similar(positive=['woman', 'prince'], negative=['man'], topn=1)\n",
    "print(result)\n",
    "result = model.wv.most_similar(positive=['splendid', 'horrible'], negative=['good'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the above result for the GoogleWord2Vec Model and our trained Word2Vec Model. We can see that for a specific case where we are comparing \"Card\" to words like \"Poker\", \"Credit\" and \"Birthday\", our word2Vec Model is able encode Semantic Similarities better as words like this might have appeared in the Reviews in the same context. \n",
    "### The case when we are trying to generalize comparisions such as woman+king-man, woman+prince-man and splendid+horrible-good, we can see that the GoogleWord2Vec Model does better in giving us a more meaningful result(queen, princess and hideous) because it's trained on a much bigger and richer dataset. Comparing this to our Word2Vec model, we can clearly see that the dataset used to train the model is inadequate and is very specific to make such general comparisions and gives us results which don't make semantic sense (wallace, groomsman, unboxing) as the GoogleWord2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing empty Reviews from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df_sampled[df_sampled['review_body'].map(lambda d: len(d)) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Binary Dataset by removing Netural Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary = df_sampled[df_sampled['Sentiment'] != 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split for Binary and Teranry Datasets (80% Training, 20% Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(df_binary['review_body'],df_binary['Sentiment'], test_size=0.2, random_state=0)\n",
    "X_train_ternary, X_test_ternary, y_train_ternary, y_test_ternary = train_test_split(df_sampled['review_body'],df_sampled['Sentiment'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating TF-IDF Vectors for Reviews \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSentences(review):\n",
    "    return \" \".join(review)\n",
    "\n",
    "X_train_binary_sentences = X_train_binary.apply(lambda review : makeSentences(review))\n",
    "X_test_binary_sentences = X_test_binary.apply(lambda review : makeSentences(review))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "X_train_binary_sentences = vectorizer.fit_transform(X_train_binary_sentences)\n",
    "X_test_binary_sentences = vectorizer.transform(X_test_binary_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Driver Function to Create all the Required Datasets for the Upcoming Models\n",
    "#### The below function is used to create 3 different types of Datasets\n",
    "#### 1. Average Word Vector for the Entire Review\n",
    "#### 2. First 10 Word Vectors for Each Review(if a review is less than 10 words, pad it with zeros, or if it's more than 10 words limit it to 10 words). Shape (3000)\n",
    "#### 3. First 50 Word Vectors for Each Review(if a review is less than 50 words, pad it with zeros, or if it's more than 50 words limit it to 50 words). Shape (50,300)\n",
    "\n",
    "#### We use the below function to create datasets for (Binary, Ternary) * (GoogleWord2Vec, myWord2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def word2VecMap(data, word2VecModel):\n",
    "    vector_list = []\n",
    "    mean_vector_list = []\n",
    "    vector_list_10 = []\n",
    "    vector_list_RNN = []\n",
    "    for sentence in data:\n",
    "        #print(sentence)\n",
    "        temp = []\n",
    "        temp_10=[]\n",
    "        temp_50=[]\n",
    "        for word in sentence:\n",
    "            #print(word)\n",
    "            embedding = word2VecModel[word]\n",
    "            temp.append(embedding)\n",
    "            temp_10.append(embedding)\n",
    "            temp_50.append(embedding)\n",
    "        #vector_list.append(temp)\n",
    "        mean_vector_list.append(np.mean(temp,axis=0))\n",
    "        length_10 = len(temp_10)\n",
    "        length_50 = len(temp_50)\n",
    "        if(length_10<10):\n",
    "            zero_vector = np.zeros(300, dtype=float)\n",
    "            zero_vector = zero_vector.tolist()\n",
    "            for i in range(10-length_10):\n",
    "                temp_10.append(zero_vector)\n",
    "        if(length_10>10):\n",
    "            temp_10 = temp_10[:10]\n",
    "        if(length_50<50):\n",
    "            zero_vector = np.zeros(300, dtype=float)\n",
    "            zero_vector = zero_vector.tolist()\n",
    "            for i in range(50-length_50):\n",
    "                temp_50.append(zero_vector)\n",
    "        if(length_50>50):\n",
    "            temp_50 = temp_50[:50]\n",
    "        vector_list_10.append(np.reshape(temp_10,(3000)))\n",
    "        vector_list_RNN.append(temp_50)\n",
    "    return mean_vector_list, vector_list_10, vector_list_RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary GoogleWord2Vec Word Embedding Dataset (Average, 10 word and 50 word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_binary_google_vector_mean, X_train_binary_google_vector_10, X_train_binary_google_vector_50 = word2VecMap(X_train_binary, wv)\n",
    "X_test_binary_google_vector_mean,X_test_binary_google_vector_10, X_test_binary_google_vector_50 = word2VecMap(X_test_binary, wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teranry GoogleWord2Vec Word Embedding Dataset (Average, 10 word and 50 word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ternary_google_vector_mean, X_train_ternary_google_vector_10, X_train_ternary_google_vector_50 = word2VecMap(X_train_ternary, wv)\n",
    "X_test_ternary_google_vector_mean, X_test_ternary_google_vector_10, X_test_ternary_google_vector_50 = word2VecMap(X_test_ternary, wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary myWord2Vec Word Embedding Dataset (Average, 10 word and 50 word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_binary_myWord2Vec_vector_mean, X_train_binary_myWord2Vec_vector_10, X_train_binary_myWord2Vec_vector_50 = word2VecMap(X_train_binary, model.wv)\n",
    "X_test_binary_myWord2Vec_vector_mean,X_test_binary_myWord2Vec_vector_10, X_test_binary_myWord2Vec_vector_50  = word2VecMap(X_test_binary, model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ternary myWord2Vec Word Embedding Dataset (Average, 10 word and 50 word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ternary_myWord2Vec_vector_mean, X_train_ternary_myWord2Vec_vector_10, X_train_ternary_myWord2Vec_vector_50 = word2VecMap(X_train_ternary, model.wv)\n",
    "X_test_ternary_myWord2Vec_vector_mean, X_test_ternary_myWord2Vec_vector_10, X_test_ternary_myWord2Vec_vector_50 = word2VecMap(X_test_ternary, model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "clf = Perceptron(random_state=36)\n",
    "\n",
    "clf.fit(X_train_binary_google_vector_mean, y_train_binary)\n",
    "\n",
    "y_pred_train = clf.predict(X_train_binary_google_vector_mean)\n",
    "accuracy_train = accuracy_score(y_train_binary, y_pred_train)\n",
    "precision_score_train = precision_score(y_train_binary, y_pred_train)\n",
    "recall_score_train = recall_score(y_train_binary, y_pred_train)\n",
    "f1_score_train = f1_score(y_train_binary, y_pred_train)\n",
    "\n",
    "y_pred_test= clf.predict(X_test_binary_google_vector_mean)\n",
    "accuracy_test = accuracy_score(y_test_binary, y_pred_test)\n",
    "precision_score_test = precision_score(y_test_binary, y_pred_test)\n",
    "recall_score_test = recall_score(y_test_binary, y_pred_test)\n",
    "f1_score_test = f1_score(y_test_binary, y_pred_test)\n",
    "\n",
    "print(\"Accuracy %2.4f for Perceptron on Binary-Google\" % (accuracy_test))\n",
    "\n",
    "clf = Perceptron(random_state=4)\n",
    "\n",
    "clf.fit(X_train_binary_myWord2Vec_vector_mean, y_train_binary)\n",
    "\n",
    "y_pred_train = clf.predict(X_train_binary_myWord2Vec_vector_mean)\n",
    "accuracy_train = accuracy_score(y_train_binary, y_pred_train)\n",
    "precision_score_train = precision_score(y_train_binary, y_pred_train)\n",
    "recall_score_train = recall_score(y_train_binary, y_pred_train)\n",
    "f1_score_train = f1_score(y_train_binary, y_pred_train)\n",
    "\n",
    "y_pred_test= clf.predict(X_test_binary_myWord2Vec_vector_mean)\n",
    "accuracy_test = accuracy_score(y_test_binary, y_pred_test)\n",
    "precision_score_test = precision_score(y_test_binary, y_pred_test)\n",
    "recall_score_test = recall_score(y_test_binary, y_pred_test)\n",
    "f1_score_test = f1_score(y_test_binary, y_pred_test)\n",
    "\n",
    "print(\"Accuracy %2.4f for Perceptron on Binary-myWord2Vec\" % (accuracy_test))\n",
    "\n",
    "clf = Perceptron(random_state=0)\n",
    "\n",
    "clf.fit(X_train_binary_sentences, y_train_binary)\n",
    "\n",
    "y_pred_train = clf.predict(X_train_binary_sentences)\n",
    "accuracy_train = accuracy_score(y_train_binary, y_pred_train)\n",
    "precision_score_train = precision_score(y_train_binary, y_pred_train)\n",
    "recall_score_train = recall_score(y_train_binary, y_pred_train)\n",
    "f1_score_train = f1_score(y_train_binary, y_pred_train)\n",
    "\n",
    "y_pred_test= clf.predict(X_test_binary_sentences)\n",
    "accuracy_test = accuracy_score(y_test_binary, y_pred_test)\n",
    "precision_score_test = precision_score(y_test_binary, y_pred_test)\n",
    "recall_score_test = recall_score(y_test_binary, y_pred_test)\n",
    "f1_score_test = f1_score(y_test_binary, y_pred_test)\n",
    "\n",
    "print(\"Accuracy %2.4f for Perceptron on Binary- TfIdf\" % (accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron (Binary-Google) - 80%\n",
    "### Perceptron (Binary-myWord2Vec) - 81.40%\n",
    "### Perceptron (Binary- TfIdf) - 81.65%\n",
    "\n",
    "#### We can see that the TF-IDF feature dataset did better than the word2Vec embeddings with respect to using the perceptron as the classifier. We can all see that our word2Vec model gave us a better accuracy when compared to the GoogleWord2Vec model and this might be because our word2vec model is trained specificially on the dataset we are using so it's able to model the entire dataset better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC()\n",
    "\n",
    "clf.fit(X_train_binary_google_vector_mean, y_train_binary)\n",
    "\n",
    "y_pred_train = clf.predict(X_train_binary_google_vector_mean)\n",
    "accuracy_train = accuracy_score(y_train_binary, y_pred_train)\n",
    "precision_score_train = precision_score(y_train_binary, y_pred_train)\n",
    "recall_score_train = recall_score(y_train_binary, y_pred_train)\n",
    "f1_score_train = f1_score(y_train_binary, y_pred_train)\n",
    "\n",
    "y_pred_test= clf.predict(X_test_binary_google_vector_mean)\n",
    "accuracy_test = accuracy_score(y_test_binary, y_pred_test)\n",
    "precision_score_test = precision_score(y_test_binary, y_pred_test)\n",
    "recall_score_test = recall_score(y_test_binary, y_pred_test)\n",
    "f1_score_test = f1_score(y_test_binary, y_pred_test)\n",
    "\n",
    "print(\"Accuracy %2.4f Precision %2.4f Recall %2.4f and f1-score %2.4f for SVM on test data\" % (accuracy_test, precision_score_test, recall_score_test, f1_score_test))\n",
    "\n",
    "clf = LinearSVC()\n",
    "\n",
    "clf.fit(X_train_binary_myWord2Vec_vector_mean, y_train_binary)\n",
    "\n",
    "y_pred_train = clf.predict(X_train_binary_myWord2Vec_vector_mean)\n",
    "accuracy_train = accuracy_score(y_train_binary, y_pred_train)\n",
    "precision_score_train = precision_score(y_train_binary, y_pred_train)\n",
    "recall_score_train = recall_score(y_train_binary, y_pred_train)\n",
    "f1_score_train = f1_score(y_train_binary, y_pred_train)\n",
    "\n",
    "y_pred_test= clf.predict(X_test_binary_myWord2Vec_vector_mean)\n",
    "accuracy_test = accuracy_score(y_test_binary, y_pred_test)\n",
    "precision_score_test = precision_score(y_test_binary, y_pred_test)\n",
    "recall_score_test = recall_score(y_test_binary, y_pred_test)\n",
    "f1_score_test = f1_score(y_test_binary, y_pred_test)\n",
    "\n",
    "print(\"Accuracy %2.4f Precision %2.4f Recall %2.4f and f1-score %2.4f for SVM on test data\" % (accuracy_test, precision_score_test, recall_score_test, f1_score_test))\n",
    "\n",
    "clf = LinearSVC()\n",
    "\n",
    "clf.fit(X_train_binary_sentences, y_train_binary)\n",
    "\n",
    "y_pred_train = clf.predict(X_train_binary_sentences)\n",
    "accuracy_train = accuracy_score(y_train_binary, y_pred_train)\n",
    "precision_score_train = precision_score(y_train_binary, y_pred_train)\n",
    "recall_score_train = recall_score(y_train_binary, y_pred_train)\n",
    "f1_score_train = f1_score(y_train_binary, y_pred_train)\n",
    "\n",
    "y_pred_test= clf.predict(X_test_binary_sentences)\n",
    "accuracy_test = accuracy_score(y_test_binary, y_pred_test)\n",
    "precision_score_test = precision_score(y_test_binary, y_pred_test)\n",
    "recall_score_test = recall_score(y_test_binary, y_pred_test)\n",
    "f1_score_test = f1_score(y_test_binary, y_pred_test)\n",
    "\n",
    "print(\"Accuracy %2.4f Precision %2.4f Recall %2.4f and f1-score %2.4f for SVM on test data\" % (accuracy_test, precision_score_test, recall_score_test, f1_score_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (Binary-Google) - 81.74%\n",
    "### SVM (Binary-myWord2Vec) - 84.61%\n",
    "### SVM (Binary- TfIdf) - 86.99%\n",
    "\n",
    "#### We can see that this is the exact same case as the Perceptron where the TF-IDF feature set did better than the word2Vec models and also our word2Vec Model did better than the GoogleWord2Vec Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNN\n",
    "### References: https://medium.com/analytics-vidhya/pytorch-for-deep-learning-binary-classification-logistic-regression-382abd97fb43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the Mean vector of a Review for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_binary_google_vector_mean_scaled = sc.fit_transform(X_train_binary_google_vector_mean)\n",
    "X_test_binary_google_vector_mean_scaled = sc.transform(X_test_binary_google_vector_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ternary_google_vector_mean_scaled = sc.fit_transform(X_train_ternary_google_vector_mean)\n",
    "X_test_ternary_google_vector_mean_scaled = sc.transform(X_test_ternary_google_vector_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_binary_myWord2Vec_vector_mean_scaled = sc.fit_transform(X_train_binary_myWord2Vec_vector_mean)\n",
    "X_test_binary_myWord2Vec_vector_mean_scaled = sc.transform(X_test_binary_myWord2Vec_vector_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ternary_myWord2Vec_vector_mean_scaled = sc.fit_transform(X_train_ternary_myWord2Vec_vector_mean)\n",
    "X_test_ternary_myWord2Vec_vector_mean_scaled = sc.transform(X_test_ternary_myWord2Vec_vector_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Class(dataset) for Datasets which derives from the Dataset class of Pytorch and which implements basic functions such as lookup, length and converting to Pytorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "  def __init__(self,x,y):\n",
    "    self.x = torch.tensor(x,dtype=torch.float32)\n",
    "    self.y = torch.tensor(y,dtype=torch.float32)\n",
    "    self.length = self.x.shape[0]\n",
    " \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x[idx],self.y[idx]\n",
    "  def __len__(self):\n",
    "    return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting our datasets to Dataloader classes to enable batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = dataset(X_train_binary_google_vector_mean_scaled,np.array(y_train_binary))\n",
    "train_loader = DataLoader(trainset,batch_size=64,shuffle=False)\n",
    "\n",
    "testset = dataset(X_test_binary_google_vector_mean_scaled,np.array(y_test_binary))\n",
    "test_loader = DataLoader(testset,batch_size=1,shuffle=False)\n",
    "\n",
    "trainset_myWord2Vec = dataset(X_train_binary_myWord2Vec_vector_mean_scaled,np.array(y_train_binary))\n",
    "train_loader_myWord2Vec = DataLoader(trainset_myWord2Vec,batch_size=64,shuffle=False)\n",
    "\n",
    "testset_myWord2Vec = dataset(X_test_binary_myWord2Vec_vector_mean_scaled,np.array(y_test_binary))\n",
    "test_loader_myWord2Vec = DataLoader(testset_myWord2Vec,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN for Binary Classification\n",
    "#### Input Layer Dimension: 300\n",
    "#### Hidden Layer Dimension: 50\n",
    "#### Output Layer Dimension: 1\n",
    "#### Batch Normalisation Layer 1:  50\n",
    "#### Batch Normalisation Layer 2: 10\n",
    "#### Using Relu between Input and Hidden layers and Hidden layer and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(300, 50) \n",
    "        self.layer_2 = nn.Linear(50, 10)\n",
    "        self.layer_out = nn.Linear(10, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(50)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(10)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNN Google Binary Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.001\n",
    "### Epcohs = 10\n",
    "### Optimiser = torch.optim.Adam\n",
    "### Loss Function = Binary Cross Entropy with Logits Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "model_google = Net()\n",
    "model_google.to(device)\n",
    "optimizer = torch.optim.Adam(model_google.parameters(),lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to get accuracy for binary classes\n",
    "#### 1. Apply Sigmoid on the predicted value to squash it between 0 and 1\n",
    "#### 2. Then round of the value to either 1 or 0\n",
    "#### 3. Compare with the actual value and compute accuracy by dividing total number of correctly predicted values with total number of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = acc * 100\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "#### Outer for loop runs for number of epochs specified\n",
    "#### Inner for loop runs for number of batches in Train set\n",
    "#### Predict values within the inner for loop for each batch by computing loss, backpropgating it and zeroing out the gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_google.train()\n",
    "for epoch in range(epochs):    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #print(X_batch.size())\n",
    "        y_pred = model_google(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model \n",
    "#### Make sure the gradients are not updated by computing test set values under torch.no_grad()\n",
    "#### Get the list of predicted values and compare against actual values to get the test accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [] \n",
    "\n",
    "model_google.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch,y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model_google(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_matrix(y_test_binary, y_pred_list))\n",
    "print(accuracy_score(y_test_binary, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN(Average-Binary-Google) - 84.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNN myWord2Vec Binary Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.001\n",
    "### Epcohs = 10\n",
    "### Optimiser = torch.optim.Adam\n",
    "### Loss Function = Binary Cross Entropy with Logits Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "model_myWord2Vec = Net()\n",
    "model_myWord2Vec.to(device)\n",
    "optimizer = torch.optim.Adam(model_myWord2Vec.parameters(),lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#print(model_myWord2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_myWord2Vec.train()\n",
    "for epoch in range(epochs):    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    for X_batch, y_batch in train_loader_myWord2Vec:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model_myWord2Vec(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_loader_myWord2Vec):.5f} | Acc: {epoch_acc/len(train_loader_myWord2Vec):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [] \n",
    "\n",
    "model_myWord2Vec.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch,y_batch in test_loader_myWord2Vec:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model_myWord2Vec(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_matrix(y_test_binary, y_pred_list))\n",
    "print(accuracy_score(y_test_binary, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN(Average-Binary-myWord2Vec) - 86.25%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNN Google Ternary Average\n",
    "### Refrences: https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN for Ternary Classification\n",
    "#### Input Layer Dimension: 300\n",
    "#### Hidden Layer Dimension: 50\n",
    "#### Output Layer Dimension: 3\n",
    "#### Batch Normalisation Layer 1:  50\n",
    "#### Batch Normalisation Layer 2: 10\n",
    "#### Using Relu between Input and Hidden layers, Hidden layer and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_ternary = dataset(X_train_ternary_google_vector_mean_scaled,np.array(y_train_ternary))\n",
    "train_loader_ternary = DataLoader(trainset_ternary,batch_size=128,shuffle=False)\n",
    "\n",
    "testset_ternary = dataset(X_test_ternary_google_vector_mean_scaled,np.array(y_test_ternary))\n",
    "test_loader_ternary = DataLoader(testset_ternary,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "class MultiClassNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiClassNN, self).__init__()\n",
    "        # Number of input features is 300.\n",
    "        self.layer_1 = nn.Linear(300, 50) \n",
    "        self.layer_2 = nn.Linear(50, 10)\n",
    "        self.layer_out = nn.Linear(10, 3) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(50)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(10)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.layer_1(inputs)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm1(x)\n",
    "\n",
    "        \n",
    "        x = self.layer_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_out(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to get accuracy for Ternary classes\n",
    "#### 1. Apply Softmax on the predicted value to get probablites of each class\n",
    "#### 2. Get the index of the class with maximum probability\n",
    "#### 3. Compare with the actual value and compute accuracy by dividing total number of correctly predicted values with total number of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.01\n",
    "### Epcohs = 50\n",
    "### Optimiser = torch.optim.Adam\n",
    "### Loss Function = Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "\n",
    "model_ternary = MultiClassNN()\n",
    "model_ternary.to(device)\n",
    "optimizer = torch.optim.Adam(model_ternary.parameters(),lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#print(model_ternary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ternary.train()\n",
    "for epoch in range(epochs):    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    for X_batch, y_batch in train_loader_ternary:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        y_pred = model_ternary(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.long())\n",
    "        acc = multi_acc(y_pred, y_batch.long())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_loader_ternary):.5f} | Acc: {epoch_acc/len(train_loader_ternary):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [] \n",
    "\n",
    "model_ternary.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader_ternary:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model_ternary(X_batch)\n",
    "        _, y_pred_tags = torch.max(y_test_pred, dim = 1)\n",
    "        y_pred_list.append(y_pred_tags.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_matrix(y_test_ternary, y_pred_list))\n",
    "print(accuracy_score(y_test_ternary, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN(Average-Ternary-Google) - 67.97%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  FNN myWord2Vec Ternary Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_ternary_myWord2Vec = dataset(X_train_ternary_myWord2Vec_vector_mean_scaled,np.array(y_train_ternary))\n",
    "train_loader_ternary_myWord2Vec = DataLoader(trainset_ternary_myWord2Vec,batch_size=128,shuffle=False)\n",
    "\n",
    "testset_ternary_myWord2Vec = dataset(X_test_ternary_myWord2Vec_vector_mean_scaled,np.array(y_test_ternary))\n",
    "test_loader_ternary_myWord2Vec = DataLoader(testset_ternary_myWord2Vec,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.01\n",
    "### Epochs = 50\n",
    "### Optimiser = torch.optim.Adam\n",
    "### Loss Function = Cross Entropy Loss¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "\n",
    "model_ternary_myWord2Vec = MultiClassNN()\n",
    "model_ternary_myWord2Vec.to(device)\n",
    "optimizer = torch.optim.Adam(model_ternary_myWord2Vec.parameters(),lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#print(model_ternary_myWord2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ternary_myWord2Vec.train()\n",
    "for epoch in range(epochs):    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    for X_batch, y_batch in train_loader_ternary_myWord2Vec:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)        \n",
    "        y_pred = model_ternary_myWord2Vec(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.long())\n",
    "        acc = multi_acc(y_pred, y_batch.long())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_loader_ternary_myWord2Vec):.5f} | Acc: {epoch_acc/len(train_loader_ternary_myWord2Vec):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [] \n",
    "\n",
    "model_ternary_myWord2Vec.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader_ternary_myWord2Vec:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model_ternary_myWord2Vec(X_batch)\n",
    "        _, y_pred_tags = torch.max(y_test_pred, dim = 1)\n",
    "        y_pred_list.append(y_pred_tags.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_matrix(y_test_ternary, y_pred_list))\n",
    "print(accuracy_score(y_test_ternary, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN(Average-Ternary-myWord2Vec) - 71.1%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN(Average-Binary-Google) - 84.2%\n",
    "### FNN(Average-Binary-myWord2Vec) - 86.25%\n",
    "### FNN(Average-Ternary-Google) - 67.97%\n",
    "### FNN(Average-Ternary-myWord2Vec) - 71.1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When comparing the accuracy values of the Simple models with the Feed Forward Neural Networks(Binary Classification) we can see that the FNN did better in terms of the overall accuracy (86.25% for myWord2Vec and 84.2% for Google Word2Vec) compared to the Perceptron(81.40% for myWord2Vec and 80% for Google Word2Vec) and SVM(84.61% for myWord2Vec and 81.74% for Google Word2Vec) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Ternary Classification models have a lower accuracy when compared to the binary models because of the ambiguity of the netural class. Intuitively, it's difficult to determine if a review is neutral as when you write a review, it's usually talking about the pros and cons of the product. This explains why even our models are not able to pick this diffrentiation up succesfully. We are also limited to a lower number of neutral reviews (50k) compared to Postive and Negative Revies(100k each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNN 10 Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_10 = dataset(X_train_binary_google_vector_10,np.array(y_train_binary))\n",
    "train_loader_10 = DataLoader(trainset_10,batch_size=64,shuffle=False)\n",
    "\n",
    "testset_10 = dataset(X_test_binary_google_vector_10,np.array(y_test_binary))\n",
    "test_loader_10 = DataLoader(testset_10,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN for Binary Classification of 10 word reviews\n",
    "#### Input Layer Dimension: 3000 (10x300 tensor is reshaped to 3000)\n",
    "#### Hidden Layer Dimension: 50\n",
    "#### Output Layer Dimension: 1\n",
    "#### Batch Normalisation Layer 1:  50\n",
    "#### Batch Normalisation Layer 2: 10\n",
    "#### Using Relu between Input and Hidden layers and Hidden layer and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_10, self).__init__()\n",
    "        self.layer_1 = nn.Linear(3000, 50) \n",
    "        self.layer_2 = nn.Linear(50, 10)\n",
    "        self.layer_out = nn.Linear(10, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(50)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(10)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNN Google Binary 10Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.0001\n",
    "### Epochs = 10\n",
    "### Optimiser = torch.optim.Adam¶\n",
    "### Loss Function = Binary Cross Entropy with Logits Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "epochs = 10\n",
    "\n",
    "model_google_10 = Net_10()\n",
    "model_google_10.to(device)\n",
    "optimizer = torch.optim.Adam(model_google_10.parameters(),lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_google_10.train()\n",
    "for epoch in range(epochs):    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    for X_batch, y_batch in train_loader_10:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #print(X_batch.size())\n",
    "        y_pred = model_google_10(X_batch)\n",
    "        #print(y_pred)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_loader_10):.5f} | Acc: {epoch_acc/len(train_loader_10):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [] \n",
    "\n",
    "model_google_10.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch,y_batch in test_loader_10:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model_google_10(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_matrix(y_test_binary, y_pred_list))\n",
    "print(accuracy_score(y_test_binary, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN(10Word-Binary-Google) - 75.15%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNN myWord2Vec Binary 10Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_myWord2Vec_10 = dataset(X_train_binary_myWord2Vec_vector_10,np.array(y_train_binary))\n",
    "train_loader_myWord2Vec_10 = DataLoader(trainset_myWord2Vec_10,batch_size=32,shuffle=False)\n",
    "\n",
    "testset_myWord2Vec_10 = dataset(X_test_binary_myWord2Vec_vector_10,np.array(y_test_binary))\n",
    "test_loader_myWord2Vec_10 = DataLoader(testset_myWord2Vec_10,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.0001\n",
    "### Epochs = 10\n",
    "### Optimiser = torch.optim.Adam\n",
    "### Loss Function = Binary Cross Entropy with Logits Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "epochs = 10\n",
    "\n",
    "model_myWord2Vec_10 = Net_10()\n",
    "model_myWord2Vec_10.to(device)\n",
    "optimizer = torch.optim.Adam(model_myWord2Vec_10.parameters(),lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_google_10.train()\n",
    "for epoch in range(epochs):    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    for X_batch, y_batch in train_loader_myWord2Vec_10:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #print(X_batch.size())\n",
    "        y_pred = model_myWord2Vec_10(X_batch)\n",
    "        #print(y_pred)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_loader_myWord2Vec_10):.5f} | Acc: {epoch_acc/len(train_loader_myWord2Vec_10):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [] \n",
    "\n",
    "model_myWord2Vec_10.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch,y_batch in test_loader_myWord2Vec_10:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model_myWord2Vec_10(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_matrix(y_test_binary, y_pred_list))\n",
    "print(accuracy_score(y_test_binary, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN(10Word-Binary-myWord2Vec) - 76.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNN Google Ternary 10Word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_google_10_ternary = dataset(X_train_ternary_google_vector_10,np.array(y_train_ternary))\n",
    "train_loader_google_10_ternary= DataLoader(trainset_google_10_ternary,batch_size=32,shuffle=False)\n",
    "\n",
    "testset_google_10_ternary = dataset(X_test_ternary_google_vector_10,np.array(y_test_ternary))\n",
    "test_loader_google_10_ternary = DataLoader(testset_google_10_ternary,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN for Ternary Classification of 10 word reviews\n",
    "#### Input Layer Dimension: 3000 (10x300 tensor is reshaped to 3000)\n",
    "#### Hidden Layer Dimension: 50\n",
    "#### Output Layer Dimension: 3\n",
    "#### Batch Normalisation Layer 1:  50\n",
    "#### Batch Normalisation Layer 2: 10\n",
    "#### Using Relu between Input and Hidden layers and Hidden layer and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassNN_10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiClassNN_10, self).__init__()\n",
    "        self.layer_1 = nn.Linear(3000, 50) \n",
    "        self.layer_2 = nn.Linear(50, 10)\n",
    "        self.layer_out = nn.Linear(10, 3) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(50)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(10)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.layer_1(inputs)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm1(x)\n",
    "\n",
    "        \n",
    "        x = self.layer_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_out(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.001\n",
    "### Epochs = 10\n",
    "### Optimiser = torch.optim.Adam\n",
    "### Loss Function = Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "model_ternary_10 = MultiClassNN_10()\n",
    "model_ternary_10.to(device)\n",
    "optimizer = torch.optim.Adam(model_ternary_10.parameters(),lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#print(model_ternary_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ternary_10.train()\n",
    "for epoch in range(epochs):    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    for X_batch, y_batch in train_loader_google_10_ternary:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        y_pred = model_ternary_10(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.long())\n",
    "        acc = multi_acc(y_pred, y_batch.long())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_loader_google_10_ternary):.5f} | Acc: {epoch_acc/len(train_loader_google_10_ternary):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [] \n",
    "\n",
    "model_ternary_10.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader_google_10_ternary:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model_ternary_10(X_batch)\n",
    "        _, y_pred_tags = torch.max(y_test_pred, dim = 1)\n",
    "        y_pred_list.append(y_pred_tags.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_matrix(y_test_ternary, y_pred_list))\n",
    "print(accuracy_score(y_test_ternary, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN(10Word-Ternary-Google) - 60.1%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNN myWord2Vec Ternary 10Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_myWord2Vec_10_ternary = dataset(X_train_ternary_myWord2Vec_vector_10,np.array(y_train_ternary))\n",
    "train_loader_myWord2Vec_10_ternary= DataLoader(trainset_myWord2Vec_10_ternary,batch_size=32,shuffle=False)\n",
    "\n",
    "testset_myWord2Vec_10_ternary = dataset(X_test_ternary_myWord2Vec_vector_10,np.array(y_test_ternary))\n",
    "test_loader_myWord2Vec_10_ternary = DataLoader(testset_myWord2Vec_10_ternary,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.001\n",
    "### Epochs = 10\n",
    "### Optimiser = torch.optim.Adam\n",
    "### Loss Function = Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "model_ternary_myWord2Vec_10 = MultiClassNN_10()\n",
    "model_ternary_myWord2Vec_10.to(device)\n",
    "optimizer = torch.optim.Adam(model_ternary_myWord2Vec_10.parameters(),lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#print(model_ternary_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ternary_myWord2Vec_10.train()\n",
    "for epoch in range(epochs):    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    for X_batch, y_batch in train_loader_myWord2Vec_10_ternary:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        y_pred = model_ternary_myWord2Vec_10(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.long())\n",
    "        acc = multi_acc(y_pred, y_batch.long())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_loader_myWord2Vec_10_ternary):.5f} | Acc: {epoch_acc/len(train_loader_myWord2Vec_10_ternary):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [] \n",
    "\n",
    "model_ternary_myWord2Vec_10.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader_myWord2Vec_10_ternary:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model_ternary_myWord2Vec_10(X_batch)\n",
    "        _, y_pred_tags = torch.max(y_test_pred, dim = 1)\n",
    "        y_pred_list.append(y_pred_tags.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_matrix(y_test_ternary, y_pred_list))\n",
    "print(accuracy_score(y_test_ternary, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN(10Word-Ternary-myWord2Vec) - 61.7%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN(10Word-Binary-Google) - 75.15%\n",
    "### FNN(10Word-Binary-myWord2Vec) - 76.8%\n",
    "### FNN(10Word-Ternary-Google) - 60.1%\n",
    "### FNN(10Word-Ternary-myWord2Vec) - 61.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the when we use just the first 10 words of the review, the accuracy suffers. The first 10 words of the review might not be able to determine the sentiment of the review as reviews are usually much larger than 10 words. Another factor that might be affecting the accuracy is the fact that we pad the vector with zeros if it's less than 10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "### References: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_50 = dataset(X_train_binary_google_vector_50,np.array(y_train_binary))\n",
    "train_loader_50 = DataLoader(trainset_50,batch_size=64,shuffle=False)\n",
    "\n",
    "testset_50 = dataset(X_test_binary_google_vector_50,np.array(y_test_binary))\n",
    "test_loader_50 = DataLoader(testset_50,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN for Binary Classification of 50 word reviews\n",
    "#### Input Layer Dimension: 300 (shape of input - batch_size x 50 x 300)\n",
    "#### Hidden State Size Dimension: 50\n",
    "#### Output Layer Dimension: 1\n",
    "#### Using Relu between RNN and Full Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first = True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):       \n",
    "        output, hidden = self.rnn(x)\n",
    "        \n",
    "        x = self.relu(hidden.squeeze(0))\n",
    "                \n",
    "        pred = self.fc(x)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Google Binary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.0001\n",
    "### Epochs = 10\n",
    "### Optimiser = torch.optim.Adam\n",
    "### Loss Function = Binary Cross Entropy with Logits Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_dim = 300\n",
    "Hidden_dim = 50\n",
    "Output_dim = 1\n",
    "epochs = 10\n",
    "\n",
    "net_RNN = RNN(Input_dim, Hidden_dim, Output_dim)\n",
    "net_RNN = net_RNN.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net_RNN.parameters(),lr=0.0001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "#print(net_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_RNN.train()\n",
    "for epoch in range(epochs):    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    for X_batch, y_batch in train_loader_50:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred = net_RNN(X_batch).squeeze(1)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        acc = binary_acc(y_pred, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_loader_50):.5f} | Acc: {epoch_acc/len(train_loader_50):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [] \n",
    "\n",
    "net_RNN.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader_50:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = net_RNN(X_batch).squeeze(1)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "y_pred_list = [a.tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_matrix(y_test_binary, y_pred_list))\n",
    "print(accuracy_score(y_test_binary, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN (Binary-Google) - 83%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN myWord2Vec Binary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_50_myWord2Vec = dataset(X_train_binary_myWord2Vec_vector_50,np.array(y_train_binary))\n",
    "train_loader_50_myWord2Vec = DataLoader(trainset_50_myWord2Vec,batch_size=64,shuffle=False)\n",
    "\n",
    "testset_50_myWord2Vec = dataset(X_test_binary_myWord2Vec_vector_50,np.array(y_test_binary))\n",
    "test_loader_50_myWord2Vec = DataLoader(testset_50_myWord2Vec,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.0001\n",
    "### Epochs = 30\n",
    "### Optimiser = torch.optim.Adam\n",
    "### Loss Function = Binary Cross Entropy with Logits Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_dim = 300\n",
    "Hidden_dim = 50\n",
    "Output_dim = 1\n",
    "epochs = 30\n",
    "\n",
    "net_RNN_myWord2Vec = RNN(Input_dim, Hidden_dim, Output_dim)\n",
    "net_RNN_myWord2Vec = net_RNN_myWord2Vec.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net_RNN_myWord2Vec.parameters(),lr=0.0001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "#print(net_RNN_myWord2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_RNN_myWord2Vec.train()\n",
    "for epoch in range(epochs):    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    for X_batch, y_batch in train_loader_50_myWord2Vec:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred = net_RNN_myWord2Vec(X_batch).squeeze(1)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        acc = binary_acc(y_pred, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_loader_50_myWord2Vec):.5f} | Acc: {epoch_acc/len(train_loader_50_myWord2Vec):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [] \n",
    "\n",
    "net_RNN_myWord2Vec.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader_50_myWord2Vec:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = net_RNN_myWord2Vec(X_batch).squeeze(1)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "y_pred_list = [a.tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_matrix(y_test_binary, y_pred_list))\n",
    "print(accuracy_score(y_test_binary, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN (Binary-myWord2Vec) - 78.31%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Google Ternary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_50_ternary = dataset(X_train_ternary_google_vector_50,np.array(y_train_ternary))\n",
    "train_loader_50_ternary = DataLoader(trainset_50_ternary,batch_size=64,shuffle=False)\n",
    "\n",
    "testset_50_ternary = dataset(X_test_ternary_google_vector_50,np.array(y_test_ternary))\n",
    "test_loader_50_ternary = DataLoader(testset_50_ternary,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN for Ternary Classification of 50 word reviews\n",
    "#### Input Layer Dimension: 300 (shape of input - batch_size x 50 x 300)\n",
    "#### Hidden State Size Dimension: 50\n",
    "#### Output Layer Dimension: 3\n",
    "#### Using Relu between RNN and Full Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Ternary(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first = True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "                \n",
    "    def forward(self, x):       \n",
    "        output, hidden = self.rnn(x)\n",
    "        x = self.relu(hidden.squeeze(0))       \n",
    "        pred = self.fc(x)\n",
    "                \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.0001\n",
    "### Epochs = 50\n",
    "### Optimiser = torch.optim.Adam\n",
    "### Loss Function = Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_dim = 300\n",
    "Hidden_dim = 50\n",
    "Output_dim = 3\n",
    "epochs = 50\n",
    "\n",
    "net_RNN_ternary = RNN_Ternary(Input_dim, Hidden_dim, Output_dim)\n",
    "net_RNN_ternary = net_RNN_ternary.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net_RNN_ternary.parameters(),lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "#print(net_RNN_ternary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_RNN_ternary.train()\n",
    "for epoch in range(epochs):    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    for X_batch, y_batch in train_loader_50_ternary:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred = net_RNN_ternary(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.long())\n",
    "        acc = multi_acc(y_pred, y_batch.long())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_loader_50_ternary):.5f} | Acc: {epoch_acc/len(train_loader_50_ternary):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [] \n",
    "\n",
    "net_RNN_ternary.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader_50_ternary:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = net_RNN_ternary(X_batch)\n",
    "        _, y_pred_tags = torch.max(y_test_pred, dim = 1)\n",
    "        y_pred_list.append(y_pred_tags.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_matrix(y_test_ternary, y_pred_list))\n",
    "print(accuracy_score(y_test_ternary, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN(Ternary-Google) - 69%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN myWord2Vec Ternary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_50_ternary_myWord2Vec = dataset(X_train_ternary_myWord2Vec_vector_50,np.array(y_train_ternary))\n",
    "train_loader_50_ternary_myWord2Vec = DataLoader(trainset_50_ternary_myWord2Vec,batch_size=64,shuffle=False)\n",
    "\n",
    "testset_50_ternary_myWord2Vec = dataset(X_test_ternary_myWord2Vec_vector_50,np.array(y_test_ternary))\n",
    "test_loader_50_ternary_myWord2Vec = DataLoader(testset_50_ternary_myWord2Vec,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.0001\n",
    "### Epochs = 50\n",
    "### Optimiser = torch.optim.Adam\n",
    "### Loss Function = Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_dim = 300\n",
    "Hidden_dim = 50\n",
    "Output_dim = 3\n",
    "epochs = 50\n",
    "\n",
    "net_RNN_ternary_myWord2Vec = RNN_Ternary(Input_dim, Hidden_dim, Output_dim)\n",
    "net_RNN_ternary_myWord2Vec = net_RNN_ternary_myWord2Vec.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net_RNN_ternary_myWord2Vec.parameters(),lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "#print(net_RNN_ternary_myWord2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_RNN_ternary_myWord2Vec.train()\n",
    "for epoch in range(epochs):    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    for X_batch, y_batch in train_loader_50_ternary_myWord2Vec:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred = net_RNN_ternary_myWord2Vec(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.long())\n",
    "        acc = multi_acc(y_pred, y_batch.long())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_loader_50_ternary_myWord2Vec):.5f} | Acc: {epoch_acc/len(train_loader_50_ternary_myWord2Vec):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [] \n",
    "\n",
    "net_RNN_ternary_myWord2Vec.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader_50_ternary_myWord2Vec:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = net_RNN_ternary_myWord2Vec(X_batch)\n",
    "        _, y_pred_tags = torch.max(y_test_pred, dim = 1)\n",
    "        y_pred_list.append(y_pred_tags.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_matrix(y_test_ternary, y_pred_list))\n",
    "print(accuracy_score(y_test_ternary, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN(Ternary-myWord2Vec) - 61.80%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN (Binary-Google) - 83%\n",
    "### RNN (Binary-myWord2Vec) - 78.31%\n",
    "### RNN (Ternary-Google) - 69%\n",
    "### RNN (Ternary-myWord2Vec) - 61.80%\n",
    "\n",
    "## Comparing the RNN accuracies to the FNN accuracies, we can see that the accuracies are lesser in the case of RNN. This might be because of Vanishing/Exploding Gradients because of which the first few layers of the Time Step either loss out on updation (gradient is very low) or dictate the final result (gradients are very high). The gradients can be clipped manually to make sure this doesn't happen, but we can take care of this using an LSTM/GRU cell instead. We also see that in this case the GoogleWord2Vec model does better in both the binary and ternary cases. This might be because the RNN takes into account the states of the previous inputs as well and the GoogleWord2Vec model has word embeddings that is able to generalize this history of words better than our Word2Vec model word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Google Binary \n",
    "### Refrences: https://blog.floydhub.com/gru-with-pytorch/\n",
    "### https://github.com/hpanwar08/sentence-classification-pytorch/blob/master/Sentiment%20analysis%20pytorch.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU for Binary Classification of 50 word reviews\n",
    "#### Input Layer Dimension: 300 (shape of input - batch_size x 50 x 300)\n",
    "#### Number of GRU layers:  1\n",
    "#### We have to initialize a hidden layer of zeros with the shape (1, batch_size, 50) which is given as an input to the GRU cell. \n",
    "#### Hidden State Size Dimension: 50\n",
    "#### Output Layer Dimension: 1\n",
    "#### The last row of the hidden layer in our Model will have the predicted output\n",
    "#### The last row is sent through a Relu function after which it's sent to the Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first = True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h, batch_size):   \n",
    "        \n",
    "        self.h = self.init_hidden(batch_size)\n",
    "                \n",
    "        output, self.h = self.gru(x, self.h)\n",
    "                \n",
    "        pred =  self.fc(self.relu(self.h[-1]))\n",
    "        \n",
    "        return pred, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros((1,batch_size,50))).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.001\n",
    "### Epochs = 10\n",
    "### Optimiser = torch.optim.Adam\n",
    "### Loss Function = Binary Cross Entropy with Logits Loss\n",
    "### Batch Size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_dim = 300\n",
    "Hidden_dim = 50\n",
    "Output_dim = 1\n",
    "n_layers = 1\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "net_GRU = GRU(Input_dim, Hidden_dim, n_layers, Output_dim)\n",
    "net_GRU = net_GRU.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net_GRU.parameters(),lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "#print(net_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_GRU.train()\n",
    "for epoch in range(epochs):  \n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    \n",
    "    h = net_GRU.init_hidden(64)\n",
    "    \n",
    "    for X_batch, y_batch in train_loader_50:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        h = h.data\n",
    "        \n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        batch = X_batch.size(0)\n",
    "  \n",
    "        y_pred,h = net_GRU(X_batch, h, batch)\n",
    "        \n",
    "        y_pred = y_pred.squeeze(1)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        acc = binary_acc(y_pred, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_loader_50):.5f} | Acc: {epoch_acc/len(train_loader_50):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [] \n",
    "\n",
    "net_GRU.eval()\n",
    "with torch.no_grad():\n",
    "    h = net_GRU.init_hidden(64)\n",
    "    for X_batch, _ in test_loader_50:\n",
    "        X_batch = X_batch.to(device)\n",
    "        batch = X_batch.size(0)\n",
    "        y_test_pred,h = net_GRU(X_batch,h,batch)\n",
    "        y_test_pred = y_test_pred.squeeze(1)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_matrix(y_test_binary, y_pred_list))\n",
    "print(accuracy_score(y_test_binary, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU(Binary-Google) - 87%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU myWord2Vec Binary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.001\n",
    "### Epochs = 10\n",
    "### Optimiser = torch.optim.Adam\n",
    "### Loss Function = Binary Cross Entropy with Logits Loss\n",
    "### Batch Size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_dim = 300\n",
    "Hidden_dim = 50\n",
    "Output_dim = 1\n",
    "n_layers = 1\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "net_GRU_myWord2Vec = GRU(Input_dim, Hidden_dim, n_layers, Output_dim)\n",
    "net_GRU_myWord2Vec = net_GRU_myWord2Vec.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net_GRU_myWord2Vec.parameters(),lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "#print(net_GRU_myWord2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_GRU_myWord2Vec.train()\n",
    "for epoch in range(epochs):  \n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    \n",
    "    h = net_GRU_myWord2Vec.init_hidden(64)\n",
    "    \n",
    "    for X_batch, y_batch in train_loader_50_myWord2Vec:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        h = h.data\n",
    "        \n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        batch = X_batch.size(0)\n",
    "  \n",
    "        y_pred,h = net_GRU_myWord2Vec(X_batch, h, batch)\n",
    "        \n",
    "        y_pred = y_pred.squeeze(1)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        acc = binary_acc(y_pred, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_loader_50_myWord2Vec):.5f} | Acc: {epoch_acc/len(train_loader_50_myWord2Vec):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [] \n",
    "\n",
    "net_GRU_myWord2Vec.eval()\n",
    "with torch.no_grad():\n",
    "    h = net_GRU_myWord2Vec.init_hidden(64)\n",
    "    for X_batch, _ in test_loader_50_myWord2Vec:\n",
    "        X_batch = X_batch.to(device)\n",
    "        batch = X_batch.size(0)\n",
    "        y_test_pred,h = net_GRU_myWord2Vec(X_batch,h,batch)\n",
    "        y_test_pred = y_test_pred.squeeze(1)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_matrix(y_test_binary, y_pred_list))\n",
    "print(accuracy_score(y_test_binary, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU(Binary-myWord2Vec) - 86.6%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Google Ternary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU for Ternary Classification of 50 word reviews\n",
    "#### Input Layer Dimension: 300 (shape of input - batch_size x 50 x 300)\n",
    "#### Number of GRU layers:  1\n",
    "#### We have to initialize a hidden layer of zeros with the shape (1, batch_size, 50) which is given as an input to the GRU cell. \n",
    "#### Hidden State Size Dimension: 50\n",
    "#### Output Layer Dimension: 3\n",
    "#### The last row of the hidden layer in our Model will have the predicted output\n",
    "#### The last row is sent through a Relu function after which it's sent to the Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_dim = 300\n",
    "Hidden_dim = 50\n",
    "Output_dim = 3\n",
    "n_layers = 1\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "net_GRU_ternary = GRU(Input_dim, Hidden_dim, n_layers, Output_dim)\n",
    "net_GRU = net_GRU_ternary.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net_GRU_ternary.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "#print(net_GRU_ternary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.001\n",
    "### Epochs = 10\n",
    "### Optimiser = torch.optim.Adam\n",
    "### Loss Function = Cross Entropy Loss\n",
    "### Batch Size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_GRU_ternary.train()\n",
    "for epoch in range(epochs):  \n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    \n",
    "    h = net_GRU_ternary.init_hidden(64)\n",
    "    \n",
    "    for X_batch, y_batch in train_loader_50_ternary:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        h = h.data\n",
    "        \n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        batch = X_batch.size(0)\n",
    "  \n",
    "        y_pred,h = net_GRU_ternary(X_batch, h, batch)\n",
    "        \n",
    "        y_pred = y_pred.squeeze(1)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.long())\n",
    "        acc = multi_acc(y_pred, y_batch.long())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_loader_50_ternary):.5f} | Acc: {epoch_acc/len(train_loader_50_ternary):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [] \n",
    "\n",
    "net_GRU_ternary.eval()\n",
    "with torch.no_grad():\n",
    "    h = net_GRU.init_hidden(64)\n",
    "    for X_batch, _ in test_loader_50_ternary:\n",
    "        X_batch = X_batch.to(device)\n",
    "        batch = X_batch.size(0)\n",
    "        y_test_pred,h = net_GRU_ternary(X_batch, h, batch)\n",
    "        _, y_pred_tags = torch.max(y_test_pred.squeeze(1), dim = 1)\n",
    "        y_pred_list.append(y_pred_tags.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_matrix(y_test_ternary, y_pred_list))\n",
    "print(accuracy_score(y_test_ternary, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU(Ternary - Google) - 71.7%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU myWord2Vec Ternary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.001\n",
    "### Epochs = 10\n",
    "### Optimiser = torch.optim.Adam\n",
    "### Loss Function = Cross Entropy Loss\n",
    "### Batch Size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_dim = 300\n",
    "Hidden_dim = 50\n",
    "Output_dim = 3\n",
    "n_layers = 1\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "net_GRU_ternary_myWord2Vec = GRU(Input_dim, Hidden_dim, n_layers, Output_dim)\n",
    "net_GRU_ternary_myWord2Vec = net_GRU_ternary_myWord2Vec.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net_GRU_ternary_myWord2Vec.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "#print(net_GRU_ternary_myWord2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_GRU_ternary_myWord2Vec.train()\n",
    "for epoch in range(epochs):  \n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    \n",
    "    h = net_GRU_ternary_myWord2Vec.init_hidden(64)\n",
    "    \n",
    "    for X_batch, y_batch in train_loader_50_ternary_myWord2Vec:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        h = h.data\n",
    "        \n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        batch = X_batch.size(0)\n",
    "  \n",
    "        y_pred,h = net_GRU_ternary_myWord2Vec(X_batch, h, batch)\n",
    "        \n",
    "        y_pred = y_pred.squeeze(1)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.long())\n",
    "        acc = multi_acc(y_pred, y_batch.long())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    #print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_loader_50_ternary_myWord2Vec):.5f} | Acc: {epoch_acc/len(train_loader_50_ternary_myWord2Vec):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [] \n",
    "\n",
    "net_GRU_ternary_myWord2Vec.eval()\n",
    "with torch.no_grad():\n",
    "    h = net_GRU_ternary_myWord2Vec.init_hidden(64)\n",
    "    for X_batch, _ in test_loader_50_ternary_myWord2Vec:\n",
    "        X_batch = X_batch.to(device)\n",
    "        batch = X_batch.size(0)\n",
    "        y_test_pred,h = net_GRU_ternary_myWord2Vec(X_batch, h, batch)\n",
    "        _, y_pred_tags = torch.max(y_test_pred.squeeze(1), dim = 1)\n",
    "        y_pred_list.append(y_pred_tags.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_matrix(y_test_ternary, y_pred_list))\n",
    "print(accuracy_score(y_test_ternary, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU(Ternary - myWord2Vec) - 71.1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU(Binary-Google) - 87%\n",
    "### GRU(Binary-myWord2Vec) - 86.6%\n",
    "### GRU(Ternary - Google) - 71.7%\n",
    "### GRU(Ternary - myWord2Vec) - 71.1%\n",
    "\n",
    "## With just 10 epochs, we can see that the accuracies for the GRU model are much better than the RNN and it's better than all the other models we've trained till now. The GRU cell overcomes the vanishing/exploding gradient problem using the update and reset gates. The GRU model gives us the best accuracies for the Ternary Dataset as well. Just like in the case of the RNN, we can see that the GoogleWord2Vec model does better our trained Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
